1. What is Kafka -

    * It is a distributed, DISK-BASED, APPEND ONLY commit log system used for high throughput, 
      fault tolerance and event streaming.

    * It decouples the consumer and producer and allows data REPLAY

    * It uses LONG POLLING and not continuos polling like web sockets

    * Kafka is not a Database , REST service or WebSocket

    * When Kafka is used -  

        Event driven microservices
        Audit logs and activity tracking
        Asynchronous processing

2. Topic - 

    * It represents a logical stream of events for example - Order Created, payment-success, User-Regsitered
        
    * Topic ≠ Queue ≠ Table

    * Topic is append only

3. Partitions -

    * It is physical division of topics

    * It is done to achieve - Parallelism, Scalability, Throughput

    * Example - 

        Topic: order-notification
        Partitions: 3

        order-notification-0
        order-notification-1
        order-notification-2

4. Offset -

    * Logical position of a Record in a partition

    * It is never re-used, Always increasing, Not User-defined

5. Broker -

    Kafka server that stores the data is called the Broker

6. Storage Model -

    * Kafka stores data on disk using append only log segments and NOT MEMORY

    * Each log segments contains multiple - .log file, .index file etc

    * Each of these .log file stores RECORDS - which contains Offset | TimeStamp | Key | Value | Headers

    * .index file are sparse indexes which stores some offsets byte positions

    * Directory Structure -
        /kafka-logs/
        order-notification-1/
        00000000000000000000.log
        00000000000000000000.index
        00000000000000000000.timeindex

7. .log Files - 

    It is append only

    It allows Sequentially writes only

    No Updates or Delete

    It contains Records - [offset][timestamp][key][value][headers]

8. .index files -

    Kafka uses sparse indexes and stores offset byte position, one entry after every few KBs

9. Why kafka is Fast -

    * It is append only writes

    * Sequential disk IO

    * Sparse indexes

    * Batching

    * Zero copy and No random writes

10. Producers & Consumers -

    Producers (Writes data) -

        Sends Records to Kafka
        Uses Key to choose partition
        kafkaTemplate.send("order-created", orderId, event);
        Key deteremines partition and guarantees ordering per key
    
    Consumers (Reads data) -

        Reads data by parition or offset
        It doesn’t query Kafka or read by key

        * It uses long polling -

            Consumer sends FetchRequest - Kafka waits until : Data arrives OR Timeout occurs
    
11. Consumer Groups -

    * It is a logical name

    * One Partition - One Consumer 

    * Partitions = 3
      Consumers = 3
        Consumer-1 → P0
        Consumer-2 → P1
        Consumer-3 → P2

    * It is used for Horizontal scaling and Fault tolerance

12. Retention Policy & Replication Factor -

    * Kafka deletes data based on retention policies -
        Time-based (e.g., 7 days) 
        Size-based (e.g., 1TB) 
        Deletion happens at log segment level, not per message.

    * Replication Factor - No. of copies of each partition

13. Revision Example -

    * Example (Order → Notification)

        1. Order Service receives REST request
        2. Order saved to DB
        3. OrderCreated event published to Kafka (Producer)
        4. Kafka stores event on disk
        5. Notification Service polls Kafka (Consumer)
        6. Notification sent to user
        7. Offset committed

    * Topic → Partition → Log Segment → Offset 
      Producer appends → Kafka stores → Consumer polls → Offset commits

14. Useful Commands -

    * LIST -

        * ./Kafka-topics.sh --bootstrap-server localhost:9092 --list -> Lists all the topics

        * ./kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic file-uploaded -> describe topics

        * ./kafka-get-offsets.sh  --bootstrap-server localhost:9092 --topic file-uploaded -> Lists the 
        no of data in the given topic example - file-uploaded:0:5
    
    * PRODUCER -

        * ./kafka-console-producer.sh --bootstrap-server localhost:9092 --topic file-uploaded -> Produce message
            manually , now type data then press CTRL + C
    
    * CONSUMER -

        * ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic file-uploaded -> Consume from latest
          offset (Doesn't Commits Offset)
        
        * ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic file-uploaded --from-beginning ->
          Consumes from beginning (Doesn't Commits Offset)
        
        * /kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic file-uploaded --group notification-service
          consumes as a group (COMMITS OFFSETS)
    
    * RESET -
    
        * ./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group notification-service ->
        Output shows: CURRENT-OFFSET, LOG-END-OFFSET & LAG 
        
        * ./kafka-consumer-groups.sh \
        --bootstrap-server localhost:9092 \
        --group notification-service \
        --topic file-uploaded \
        --reset-offsets \
        --to-earliest  ( RESETS TO ZERO )
        --to-offset 2  ( RESETS TO 2 OFFSET)
        --by-duration PT1H ( RESETS BY 1HR )
        \--execute

        This works while the Consumer is not active i.e it must be stopped
   
    * DELETE - 

        * ./kafka-topics.sh --bootstrap-server localhost:9092 --topic file-uploaded --delete -> deletes the
          given topic

15. Spring Boot Kafka Complete Example -

    * Docker Compose File -

        services:
        kafka:
            image: apache/kafka:3.7.1
            container_name: kafka
            ports:
                - "9092:9092"
            environment:
                # KRaft core
                KAFKA_PROCESS_ROLES: broker,controller
                KAFKA_NODE_ID: 1
                KAFKA_CONTROLLER_QUORUM_VOTERS: 1@localhost:9093

                # Listeners
                KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
                KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
                KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
                KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER

                # Single node fixes
                KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
                KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
                KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1

    * Producer Config -

        ##############################  KAFKA  ######################################################

        spring.kafka.bootstrap-servers=localhost:9092
        spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
        spring.kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer

        # Disable type headers (clean JSON)
        spring.kafka.producer.properties.spring.json.add.type.headers=false ( So that kafka doesn't sends 
        class in Header and expect same class to de-serialize it)

        * Producing a Kafka Event -

            CompletableFuture<SendResult<String, FileUploadedEvent>> res = kafkaTemplate.send("file-uploaded",
                    new FileUploadedEvent(meta.getId().toString(),storageKey,meta.getOriginalFileName(),Instant.now()));
            res.thenRun(() -> {

                System.out.println("Kafka Completed");
            });
            System.out.println("Kafka completed");

    * Consumer Config -

        # ===============================
        # Kafka - Consumer Configuration
        # ===============================

        spring.kafka.bootstrap-servers=localhost:9092

        # Group
        spring.kafka.consumer.group-id=notification-service

        # Consumer deserializers
        #spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
        #spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.JsonDeserializer

        # ErrorHandlingDeserializer (MANDATORY)
        spring.kafka.consumer.key-deserializer=org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
        spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.ErrorHandlingDeserializer

        spring.kafka.consumer.properties.spring.deserializer.key.delegate.class=org.apache.kafka.common.serialization.StringDeserializer
        spring.kafka.consumer.properties.spring.deserializer.value.delegate.class=org.springframework.kafka.support.serializer.JsonDeserializer

        spring.kafka.consumer.properties.spring.json.trusted.packages=com.pixxy.noti.*

        # Explicit event type (MANDATORY since headers disabled)
        spring.kafka.consumer.properties.spring.json.value.default.type=com.pixxy.noti.FileUploadedEvent

        spring.kafka.consumer.auto-offset-reset=earliest

        * @Service
            public class FileUploadedEventConsumer {
                @KafkaListener(topics = "file-uploaded")
                public void handle(FileUploadedEvent event) {
                    System.out.println(event);
                    System.out.println("Notification triggered for user={} file={}" + event.fileName() + event.fileId());
                }
            }